{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd62388",
   "metadata": {},
   "source": [
    "# Llama3\n",
    "\n",
    "Learn more about Llama3 in Meta's [release notes!](https://ai.meta.com/blog/meta-llama-3/)\n",
    "\n",
    "Massive thank you to our friends at [Ollama](https://ollama.com/library/llama3:latest) for supporting this so quickly!\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "1. Show you how to build a RAG system with Llama3, Ollama, Weaviate, and DSPy\n",
    "2. Use DSPy's MIPRO optimizer to find the optimal RAG prompt for Llama3\n",
    "\n",
    "Please note the optimal prompt is not the same for all language models! We have recently published a blog post explaining this [here](https://weaviate.io/blog/dspy-optimizers) if interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf650430",
   "metadata": {},
   "source": [
    "### Connect to Llama3 (hosted with Ollama) and Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "llama3_ollama = dspy.OllamaLocal(model=\"llama3:8b-instruct-q5_1\", max_tokens=4000, timeout_s=480)\n",
    "\n",
    "import weaviate\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client, k=10)\n",
    "\n",
    "dspy.settings.configure(lm=llama3_ollama, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e402881",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_ollama(\"say hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genereate qa pairs\n",
    "qdict = {}\n",
    "qdict[\"What is the capital of France?\"] = \"Paris\"\n",
    "qdict[\"What is the capital of Germany?\"] = \"Berlin\"\n",
    "qdict[\"What is the capital of Italy?\"] = \"Rome\"\n",
    "qdict[\"What is the capital of Spain?\"] = \"Madrid\"\n",
    "qdict[\"What is the capital of Portugal?\"] = \"Lisbon\"\n",
    "qdict[\"What is the capital of the United Kingdom?\"] = \"London\"\n",
    "qdict[\"What is the capital of the United States?\"] = \"Washington, D.C.\"\n",
    "qdict[\"What is the capital of Canada?\"] = \"Ottawa\"\n",
    "qdict[\"What is the capital of Mexico?\"] = \"Mexico City\"\n",
    "qdict[\"What is the capital of Brazil?\"] = \"Bras√≠lia\"\n",
    "qdict[\"What is the capital of Argentina?\"] = \"Buenos Aires\"\n",
    "qdict[\"What is the capital of Chile?\"] = \"Santiago\"\n",
    "qdict[\"What is the capital of Australia?\"] = \"Canberra\"\n",
    "qdict[\"What is the capital of New Zealand?\"] = \"Wellington\"\n",
    "qdict[\"What is the capital of Japan?\"] = \"Tokyo\"\n",
    "qdict[\"What is the capital of South Korea?\"] = \"Seoul\"\n",
    "gold_answers = []\n",
    "queries = []\n",
    "for key in qdict:\n",
    "    queries.append(key)\n",
    "    gold_answers.append(qdict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977072a",
   "metadata": {},
   "source": [
    "### Load Dataset (Questions derived from Weaviate's Blog Posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# file_path = './WeaviateBlogRAG-0-0-0.json'\n",
    "# with open(file_path, 'r') as file:\n",
    "#     dataset = json.load(file)\n",
    "\n",
    "# gold_answers = []\n",
    "# queries = []\n",
    "\n",
    "# for row in dataset:\n",
    "#     gold_answers.append(row[\"gold_answer\"])\n",
    "#     queries.append(row[\"query\"])\n",
    "    \n",
    "data = []\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "    data.append(dspy.Example(gold_answer=gold_answers[i], question=queries[i]).with_inputs(\"question\"))\n",
    "\n",
    "trainset, devset, testset = data[:25], data[25:35], data[35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d58d96",
   "metadata": {},
   "source": [
    "# Metric to Assess Response Quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb88ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypedEvaluator(dspy.Signature):\n",
    "    \"\"\"Evaluate the quality of a system's answer to a question according to a given criterion.\"\"\"\n",
    "    \n",
    "    criterion: str = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    question: str = dspy.InputField(desc=\"The question asked to the system.\")\n",
    "    ground_truth_answer: str = dspy.InputField(desc=\"An expert written Ground Truth Answer to the question.\")\n",
    "    predicted_answer: str = dspy.InputField(desc=\"The system's answer to the question.\")\n",
    "    rating: float = dspy.OutputField(desc=\"A float rating between 1 and 5. IMPORTANT!! ONLY OUTPUT THE RATING!!\")\n",
    "\n",
    "\n",
    "def MetricWrapper(gold, pred, trace=None):\n",
    "    alignment_criterion = \"How aligned is the predicted_answer with the ground_truth?\"\n",
    "    return dspy.TypedPredictor(TypedEvaluator)(criterion=alignment_criterion,\n",
    "                                          question=gold.question,\n",
    "                                          ground_truth_answer=gold.gold_answer,\n",
    "                                          predicted_answer=pred.answer).rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5922bc6",
   "metadata": {},
   "source": [
    "### DSPy RAG Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Assess the the context and answer the question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"Helpful information for answering the question.\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"A detailed answer that is supported by the context. ONLY OUTPUT THE ANSWER!!\")\n",
    "    \n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=k)\n",
    "        self.generate_answer = dspy.Predict(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        pred = self.generate_answer(context=context, question=question).answer\n",
    "        return dspy.Prediction(context=context, answer=pred, question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1620c3",
   "metadata": {},
   "source": [
    "# Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a74b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RAG()(\"What is binary quantization?\").answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8cb91",
   "metadata": {},
   "source": [
    "# Compile with MIPRO\n",
    "\n",
    "What is the optimal prompt for Llama3 when answering questions about Weaviate?\n",
    "\n",
    "Starting with the prompt,\n",
    "\n",
    "`Assess the context and answer the question.`\n",
    "\n",
    "DSPy's MIPRO optimizers finds better performance with,\n",
    "\n",
    "`Given the provided context, your task is to understand the content and accurately answer the question based on the information available in the context. You should use formal English with technical terminologies where necessary and provide a detailed, relevant response.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPRO\n",
    "\n",
    "import openai\n",
    "gpt4 = dspy.OpenAI(model=\"gpt-4\", max_tokens=4000, model_type=\"chat\")\n",
    "\n",
    "teleprompter = MIPRO(prompt_model=gpt4, \n",
    "                     task_model=llama3_ollama, \n",
    "                     metric=MetricWrapper, \n",
    "                     num_candidates=3, \n",
    "                     init_temperature=0.5)\n",
    "kwargs = dict(num_threads=1, \n",
    "              display_progress=True, \n",
    "              display_table=0)\n",
    "MIPRO_compiled_RAG = teleprompter.compile(RAG(), trainset=trainset[:5], num_trials=3, max_bootstrapped_demos=1, max_labeled_demos=0, eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bbe748",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIPRO_compiled_RAG(\"what are cross encoders?\").answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_ollama.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d6f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
