{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cshorten/Library/Caches/pypoetry/virtualenvs/etl-rZpcbSY7-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to Ollama running Llama3\n",
    "import dspy\n",
    "llama3_ollama = dspy.OllamaLocal(model=\"llama3:instruct\", max_tokens=4000, timeout_s=480)\n",
    "\n",
    "dspy.settings.configure(lm=llama3_ollama)\n",
    "\n",
    "llama3_ollama(\"say hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blogs into Weaviate\n",
    "import weaviate\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "\n",
    "weaviate_client.collections.delete(\"WeaviateBlogChunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes.config as wvcc\n",
    "\n",
    "collection = weaviate_client.collections.create(\n",
    "    name=\"WeaviateBlogChunk\",\n",
    "    vectorizer_config=wvcc.Configure.Vectorizer.text2vec_cohere\n",
    "    (\n",
    "        model=\"embed-multilingual-v3.0\"\n",
    "    ),\n",
    "    properties=[\n",
    "            wvcc.Property(name=\"content\", data_type=wvcc.DataType.TEXT),\n",
    "            wvcc.Property(name=\"query\", data_type=wvcc.DataType.TEXT, skip_vectorization=True),\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182\n",
      "\n",
      "---\n",
      "title: Combining LangChain and Weaviate\n",
      "slug: combining-langchain-and-weaviate\n",
      "authors: [erika]\n",
      "date: 2023-02-21\n",
      "tags: ['integrations']\n",
      "image: ./img/hero.png\n",
      "description: \"LangChain is one of the most exciting new tools in AI. It helps overcome many limitations of LLMs, such as hallucination and limited input lengths.\"\n",
      "---\n",
      "![Combining LangChain and Weaviate](./img/hero.png)\n",
      "\n",
      "Large Language Models (LLMs) have revolutionized the way we interact and communicate with computers. These machines can understand and generate human-like language on a massive scale. LLMs are a versatile tool that is seen in many applications like chatbots, content creation, and much more. Despite being a powerful tool, LLMs have the drawback of being too general.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    for folder_name in os.listdir(main_folder_path):\n",
    "        subfolder_path = os.path.join(main_folder_path, folder_name)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            index_file_path = os.path.join(subfolder_path, 'index.mdx')\n",
    "            if os.path.isfile(index_file_path):\n",
    "                with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    sentences = split_into_sentences(content)\n",
    "                    sentence_chunks = chunk_list(sentences, 5)\n",
    "                    sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "                    blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "# Example usage\n",
    "main_folder_path = '../../datasets/weaviate-blogs'\n",
    "blog_chunks = read_and_chunk_index_files(main_folder_path)\n",
    "\n",
    "print(f\"{len(blog_chunks)}\\n\")\n",
    "print(blog_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 1182 blog chunks in 173.13162684440613 seconds.\n"
     ]
    }
   ],
   "source": [
    "from weaviate.util import get_valid_uuid\n",
    "from uuid import uuid4\n",
    "import time\n",
    "\n",
    "blogs = weaviate_client.collections.get(\"WeaviateBlogChunk\")\n",
    "\n",
    "blog_chunk_uuids = []\n",
    "\n",
    "start = time.time()\n",
    "for idx, blog_chunk in enumerate(blog_chunks):\n",
    "    id = get_valid_uuid(uuid4())\n",
    "    blog_chunk_uuids.append(id)\n",
    "    upload = blogs.data.insert(\n",
    "        properties={\n",
    "            \"content\": blog_chunk\n",
    "        },\n",
    "        uuid=id\n",
    "    )\n",
    "\n",
    "print(f\"Uploaded {len(blog_chunks)} blog chunks in {time.time() - start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Any\n",
    "import functools\n",
    "\n",
    "class UpdateProperty(dspy.Signature):\n",
    "    \"\"\"I need your help to generate the value of a property by following the instruction using the provided name-value property references. VERY IMPORTANT!! Please follow this next instruction carefully. It is EXTREMELY IMPORTANT that you only output the property value and nothing else. Do not start your response with something like `Sure, I can help with that!` or anything of the sort. JUST OUTPUT THE PROPERTY VALUE!!\n",
    "    \"\"\"\n",
    "\n",
    "    property_name: str = dspy.InputField(\n",
    "        desc=\"The name of the property that you should update.\"\n",
    "    )\n",
    "    references: str = dspy.InputField(\n",
    "        desc=\"The name-value property pairs that you should refer to while updating the property.\"\n",
    "    )\n",
    "    instruction: str = dspy.InputField(\n",
    "        desc=\"The prompt to use when generating the updated property value.\"\n",
    "    )\n",
    "    property_value: str = dspy.OutputField(\n",
    "        desc=\"The value of the updated property. VERY IMPORTANT!! ONLY OUTPUT THIS VALUE!! Do not output anything other than this value.\"\n",
    "    )\n",
    "\n",
    "class Program(dspy.Module):\n",
    "    def __init__(self, property_value_type: Any) -> None:\n",
    "        self.property_value_type = property_value_type\n",
    "        self.update_property = dspy.Predict(UpdateProperty)\n",
    "\n",
    "    def assert_property_value_type(self, property_value: str) -> bool:\n",
    "        try:\n",
    "            self.property_value_type(property_value)\n",
    "            return True\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "\n",
    "    def failed_assertion_message(self, property_name: str) -> str:\n",
    "        return f\"\"\"\n",
    "        The value of the '{property_name}' property does not match the expected type: {self.property_value_type}.\n",
    "        Please ensure that the generated value adheres to the specified type.\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, property_name: str, references: str, instruction: str) -> Any:\n",
    "        prediction: dspy.Prediction = self.update_property(\n",
    "            property_name=property_name, references=references, instruction=instruction\n",
    "        )\n",
    "\n",
    "        dspy.Suggest(\n",
    "            self.assert_property_value_type(prediction.property_value),\n",
    "            self.failed_assertion_message(property_name),\n",
    "        )\n",
    "\n",
    "        return self.property_value_type(prediction.property_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "from dspy.primitives.assertions import assert_transform_module, backtrack_handler\n",
    "\n",
    "program = Program(property_value_type=int)\n",
    "program_with_assertions = assert_transform_module(\n",
    "    program, functools.partial(backtrack_handler, max_backtracks=1)\n",
    ")\n",
    "\n",
    "property_name = \"age\"\n",
    "references = \"name: John, occupation: Engineer\"\n",
    "instruction = \"Update the 'age' property to a random integer between 25 and 35.\"\n",
    "\n",
    "result = program_with_assertions(\n",
    "    property_name=property_name, references=references, instruction=instruction\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = Program(property_value_type=str)\n",
    "\n",
    "instruction = \"\"\"\n",
    "Write a question that the content contains the answer to. IMPORTANT! Make the question specific to the content!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark GFL Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing some queries to get a sense of what this is doing...\n",
      "What are some limitations of Large Language Models (LLMs)?\n",
      "What are some limitations of LLMs and how do emerging technologies help solve them?\n",
      "What are some key concepts introduced in LangChain?\n",
      "What type of mammal lays the biggest eggs?\n",
      "What are LLM Chains?\n",
      "What is the simplest method for passing related documents from the database into the language model?\n",
      "What are the main techniques used in the LLM to generate multiple responses?\n",
      "What is the process involved in Map Rerank?\n",
      "What is an example of tool use in language models, and how can it be used to augment their capabilities?\n",
      "What is the purpose of Python REPL in LangChain?\n",
      "\n",
      "LOG: 100 queries generated in 154.4688901901245 seconds.\n",
      "\n",
      "\n",
      "LOG: 200 queries generated in 286.906888961792 seconds.\n",
      "\n",
      "\n",
      "LOG: 300 queries generated in 436.4760699272156 seconds.\n",
      "\n",
      "\n",
      "LOG: 400 queries generated in 575.7406079769135 seconds.\n",
      "\n",
      "\n",
      "LOG: 500 queries generated in 708.9861929416656 seconds.\n",
      "\n",
      "\n",
      "LOG: 600 queries generated in 844.1857469081879 seconds.\n",
      "\n",
      "\n",
      "LOG: 700 queries generated in 995.8736772537231 seconds.\n",
      "\n",
      "\n",
      "LOG: 800 queries generated in 1133.4245600700378 seconds.\n",
      "\n",
      "\n",
      "LOG: 900 queries generated in 1298.7888510227203 seconds.\n",
      "\n",
      "\n",
      "LOG: 1000 queries generated in 1445.584990978241 seconds.\n",
      "\n",
      "\n",
      "LOG: 1100 queries generated in 1588.4480819702148 seconds.\n",
      "\n",
      "1182 objects have been updated.\n"
     ]
    }
   ],
   "source": [
    "start_gfl = time.time()\n",
    "\n",
    "for idx, chunk_uuid in enumerate(blog_chunk_uuids):\n",
    "    if idx % 100 == 99:\n",
    "        print(f\"\\nLOG: {idx+1} queries generated in {time.time() - start_gfl} seconds.\\n\")\n",
    "    # Get the object\n",
    "    \n",
    "    object = blogs.query.fetch_object_by_id(chunk_uuid, return_properties=\"content\")\n",
    "    \n",
    "    # Format the references\n",
    "    \n",
    "    references=\" \".join(f\"{k}: {v}\" for k, v in object.properties.items())\n",
    "    \n",
    "    # Run GFL\n",
    "    \n",
    "    query = program(\n",
    "        property_name=\"query\",\n",
    "        references=references,\n",
    "        instruction=instruction,\n",
    "    )\n",
    "    \n",
    "    if idx < 10:\n",
    "        if idx == 0:\n",
    "            print(\"Printing some queries to get a sense of what this is doing... \\n\")\n",
    "        print(query)\n",
    "\n",
    "    # Update property in Weaviate\n",
    "    \n",
    "    blogs.data.update(\n",
    "        properties={\n",
    "            \"query\": query\n",
    "        },\n",
    "        uuid=chunk_uuid\n",
    "    )\n",
    "\n",
    "print(f\"{len(blog_chunk_uuids)} objects have been updated in {time.time() - start} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with these synthetic queries?\n",
    "\n",
    "- Benchmarking Search\n",
    "\n",
    "- Train Custom Search Models with this Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Search\n",
    "\n",
    "### WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_and_ground_truth_ids = []\n",
    "\n",
    "for chunk in blog_chunk_uuids:\n",
    "    object = blogs.query.fetch_object_by_id(chunk_uuid, return_properties=\"content\")\n",
    "    for property in object.properties:\n",
    "        # query\n",
    "        # uuid\n",
    "\n",
    "search_results = []\n",
    "for obj in queries_and_ground_truth_ids:\n",
    "    # search\n",
    "    # where is the id in the search results?\n",
    "\n",
    "# calculate recall at 1,5,10, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune Cohere Re-Ranker with Synthetic Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize query generation with PATH algorithm (https://arxiv.org/abs/2406.11706)\n",
    "\n",
    "# Use queries to train a reranker with Cohere's fine-tuning API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Recipes-GFLs",
   "language": "python",
   "name": "recipes-gfls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
